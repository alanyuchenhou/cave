\documentclass{article}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\lstset{frame=single, language=python, tabsize=4}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,automata,quotes,positioning,babel}
\usetikzlibrary{shapes.geometric, arrows}

\begin{document}
	\title{Deep Learning Approaches to Graph Mining}
	\author{Yuchen Hou}
	\maketitle

\section{Introduction}
Both academia and industry have seen pervasive adoption of deep learning since early 2010s,
when they began to outperform other machine learning techniques in various 
application domains, e.g.,
speech recognition \cite{hannun2014deep},
image recognition \cite{simonyan2014very},
natural language processing \cite{yao2013recurrent},
recommendation systems \cite{barkan2016item2vec},
and graph mining \cite{grovernode2vec}.
Deep learning can not only achieve higher prediction accuracy than other methods,
but also require much less domain knowledge and engineering.

Among those domains,
graph mining is a new and active application area for deep learning.
As deep learning has demonstrated its power in many domains,
we naturally wonder if we can apply it in graph mining,
and what kind of techniques are helpful in using this power.

My intended PhD research area is deep learning approaches to graph mining.
And my hypothesis is deep learning approaches to graph mining
will outperform most existing approaches.
Specifically, I investigate deep learning approaches to
two important types of problems in graph mining:
\begin{itemize}
	\item Predicting node and link attributes, e.g., categorical and numeric node attributes, link existence and link weight.
	\item Predicting graph attributes, e.g., the security status of a computer network and the chemical activity of a compound.
\end{itemize}
Below is the outline of this proposal:
\begin{enumerate}
	\item Introduction: I pursue my PhD research in deep learning approaches to
	 graph mining, inspired by the latest advancement of deep learning
	 in many different fields.
	\item Background: The two types of problems in graph mining I want to focus on - predicting node/link attributes and predicting graph attributes
	have very high values in various industries.
	\item Preliminary research: I experimentally demonstrate how a deep learning
	approach can effectively solve graph link weight prediction problem,
	outperforming many other existing approaches by large margins.
	\item Future research: I analyze graph attribute prediction problem, make observations based on related work and propose potential deep learning approaches to this problem.
\end{enumerate}

\section{Background}
The research in graph mining has advanced many fields
where objects and their relations can be modeled by nodes and links in a network, e.g.,
computer networks \cite{bermond1995distributed},
social networks \cite{cook2006mining},
protein interaction networks \cite{bader2003automated},
ecological food webs \cite{brown2003ecological},
and citation networks \cite{greenberg2009citation}.

An important problem in graph mining is link prediction \cite{liben2007link},
e.g., to predict whether a person will follow another person in a social network.
Link existence prediction is a well-known problem,
but one of its related problems is not: link weight prediction.
To be more specific,
the link prediction problem actually refers to link existence prediction:
to predict whether a link from a node to another node exists or will form.
Link weight prediction is
to predict the weight of a link from a node to another node
so the question it cares about is not if two nodes are connected,
but how likely or how strong they are connected.
This consideration is very practical in many scenarios.
For example, when describing a connection in a social network,
to say Alice likes Bob is not as precise as
to say Alice texts/tweets/mentions Bob 128 times per day on average,
because the second description is quantitative and more informative.
A special case of link weight prediction is collaborative filtering.
For example, to predict users' ratings to movies
is to predict the link weights on the bipartite graph
where the two disjoint node sets are users and movies.

Another important problem is graph classification \cite{duda2012pattern},
more specifically, the prediction of a categorical attribute of the graph.
This problem has extremely high value in many fields.
For example, in pharmaceutical industry,
to predict whether a compound cures or causes a certain disease
can be modeled as to predict whether a graph representing the compound
has certain chemical activities.

\section{Preliminary research}
In our preliminary research,
we want to create a technique to predict link weights in a graph.
The estimator we need should learn to represent the graph in a meaningful way
and to learn to predict the target link weights using that representation.

\subsection{Problem}
We consider the problem of link weight prediction in a weighted directed graph.
We first take a look at an example of the problem,
and then give the problem definition.
An undirected graph can be reduced to a directed graph by converting each weighted undirected link to two directed links with the same weight and opposite directions,
so the prediction for a weighted undirected graph is a special case of the problem we consider.

\subsubsection{Problem example}
Let us look at an example of link weight prediction - message volume prediction in a social network, shown in \autoref{fig:example} and \autoref{tab:example}.
In this example, there are 3 users in a social network: A, B and C.
Each user can send any amount of text messages to every other user.
We know the message volume transmitted between A and C, B and C, but not A and B.
We want to predict the message volume transmitted between A and B.
This is a simplified network similar to many real social networks, where every user interacts with other users by posting, sharing, following or liking them.
There can not be any logical approach to derive the unknown message volumes,
as they have randomness.
But there can be statistical approaches to build models to predict them.
The ability to predict these interactions potentially allows us to recommend new connections to users:
if A is predicted/expected to send a large amount of messages to B by some model,
and A is not connected to B yet,
we can recommend B as a new connection to A.

\begin{figure}[!htb]\centering
	\begin{tikzpicture}[
	node distance = 4cm,
	on grid,
	> = {Stealth[length=5pt,width=4pt]},
	every state/.style = {very thick},
	every edge quotes/.style = {sloped, anchor=north}
	]
	\node[state] (B) {B};
	\node[state] (C) [below right=of B] {C};
	\node[state] (A) [above right=of C] {A};
	\path[->]   
	(A) edge["?"]   (B)
	(B) edge[bend left,"7"]   (C)
	(B) edge[bend left,"?"]   (A)
	(A) edge[bend left,"4"]   (C)
	(C) edge[bend left,"74"]  (A)
	(C) edge[bend left,"11"]  (B);
	\end{tikzpicture}
	\caption{
		An example of link weight prediction in a weighted directed graph -
		message volume prediction in a social network.
		There are 3 users - A, B and C - in this network.
		Each user can send any amount of text messages to every other user.
		Here is what we know:
		A sent 4 messages to C,
		B sent 7 messages to C,
		and C sent 74 and 11 messages to A and B.
		What we do not know and what we want to predict is
		the message volume A and B sent to each other.
		}
	\label{fig:example}
\end{figure}
\begin{table}[!htb]\centering
	\caption{
		The same example as \autoref{fig:example}, but with edge list representation for the network.
	}
	\begin{tabularx}{0.6\textwidth}{|X|X|c|}  \hline \rowcolor{blue!40}
		Source node & Destination node & Link weight \\ \hline
		A & B & ? \\ \hline
		A & C & 74 \\ \hline
		B & A & ? \\ \hline
		B & C & 7 \\ \hline
		C & A & 4 \\ \hline
		C & B & 11 \\ \hline
	\end{tabularx}
	\label{tab:example}
\end{table}

\subsubsection{Problem definition}
Now we define the link weight prediction problem in a weighted directed graph.
\begin{itemize}
	\item Given a weighted directed graph with the node set V and link set E
	\item Build a model w = f(x, y), where x and y are nodes and w is the weight of link (x, y), that can predict the weight of any link
\end{itemize}
There is one thing to clarify about this definition.
For every possible link (1 out of $ n^2 $, where n is the number of nodes), 
if we know its weight, we know it exists;
if we do not know its weight, we do not know it exists.
This is a very practical point when we handle streaming graphs:
for any possible link,
we either know it exists and know its weight (if it has been streamed in), or we do not know if the link will ever exist, nor know its weight.

\subsection{Related work}
In our literature study on previous research,
we have found existing approaches to the link weight prediction problem
and deep learning approaches to problems related to link weight prediction problem,
but no deep learning approaches to link weight prediction problem.
In this section, we review these existing approaches.
And we use the SBM (Stochastic Block Model),
pWSBM (pure Weighted Stochastic Block Model),
bWSBM (balanced Weighted Stochastic Block Model and
DCWBM (Degree Corrected Weighted Stochastic Block Model)
baselines in the experiment evaluation.

\subsubsection{SBM (Stochastic Block Model) approach to link weight prediction}
The original SBM approach is designed for unweighted graphs and uses only link existence information \cite{holland1983stochastic}.
The main idea is to partition nodes into L groups and connect groups with bundles.
In this way, the graph has a 2-level structure:
\begin{itemize}
	\item Lower level: each group consists of nodes which were topologically similar in the original graph
	\item Upper level: groups are connected by bundles
	to represent the original graph
\end{itemize}
Given a graph with adjacency matrix A, the SBM has the following parameters:
\begin{itemize}
	\item A: link existence matrix, where $ A_{ij} \in \{0, 1\} $
	\item z: the group vector,
	where $ z_i \in \{ 1 ... L \} $ is the group label of node i
	\item $ \theta $: the bundle existence probability matrix,
	where $ \theta_{z_i z_j} $ is the existence probability of bundle ($z_i, z_j$)
\end{itemize}
So the existence of link (i, j) $ A_{ij} $ is a binary random variable following the Bernoulli distribution:
\begin{align*}
	A_{ij} \sim B(1, \theta_{z_i z_j})
\end{align*}
The SBM fits parameters z and $ \theta $
to maximize the probability of observation A:
\begin{align*}
	P(A|z, \theta) 
	= \prod_{ij} \theta_{z_i z_j}^{A_{ij}}(1-\theta_{z_i z_j})^{1-A_{ij}}
\end{align*}
which we rewrite as an exponential family:
\begin{align*}
\log(P(A|z, \theta))
= \sum_{ij} (
T(A_{ij}) \eta(\theta_{z_i z_j})
)
\end{align*}
where
\begin{align*}
T(A_{ij}) = (A_{ij}, 1)
\end{align*}
is the vector-valued function of sufficient statistics of the Bernoulli random variable and
\begin{align*}
\eta(\theta) = ( \log(\frac{\theta}{1-\theta}), \log(1-\theta) )
\end{align*}
is the vector-valued function of natural parameters of the Bernoulli random variable.

\subsubsection{pWSBM (pure Weighted Stochastic Block Model)}
The pWSBM is designed for weighted graphs and uses only link weight information \cite{aicher2014learning}.
So it differs from SBM in a few ways described below.
Here we choose model link weight with normal distribution.
Adjacency matrix A becomes the link weight matrix:
\begin{align*}
A_{ij} \in R
\end{align*}
$ \theta_{z_i z_j} $ becomes the weight distribution parameter of bundle ($z_i, z_j$):
\begin{align*}
\theta_{z_i z_j} = (\mu_{z_i z_j}, \sigma_{z_i z_j}^2)
\end{align*}
$ T(A_{ij}) $ becomes the vector-valued function of sufficient statistics of the normal random variable:
\begin{align*}
T(A_{ij}) = (A_{ij}, A_{ij}^2, 1)
\end{align*}
$ \eta(\theta) $ becomes the vector-valued function of natural parameters of the normal random variable:
\begin{align*}
\eta(\theta)
&= \eta(\mu, \sigma^2)\\
&= (\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}, -\frac{\mu^2}{2\sigma^2})
\end{align*}
So the weight of link (i, j)  $ A_{ij} $ is a real random variable following the normal distribution:
\begin{align*}
A_{ij} \sim N(\mu_{z_i z_j}, \sigma_{z_i z_j}^2)
\end{align*}
The pWSBM fits parameter z and $ \theta $
to maximize the log likelihood of observation A:
\begin{align*}
\log(P(A|z, \theta))
&= \log(P(A|z, \mu, \sigma^2))\\
&= \sum_{ij} (
A_{ij} \frac{\mu_{z_i z_j}}{\sigma_{z_i z_j}^2}
- A_{ij}^2 \frac{1}{2\sigma_{z_i z_j}^2}
- \frac{\mu_{z_i z_j}^2}{\sigma_{z_i z_j}^2}
)
\end{align*}

\subsubsection{bWSBM (balanced Weighted Stochastic Block Model)}
The bWSBM is a hybrid of SBM and pWSBM
and uses both link existence information and link weight information \cite{aicher2014learning}.
The hybrid log likelihood becomes:
\begin{align*}
&\log(P(A|z, \theta))\\
=& \alpha \sum_{ij \in E} (T_e(A_{ij}) \eta_e(\theta_{z_i z_j}))\\
& + (1 - \alpha) \sum_{ij \in W} (T_w(A_{ij}) \eta_w(\theta_{z_i z_j}))
\end{align*}
where pair $ (T_e, \eta_e) $ denotes the family of link existence distributions:
\begin{align*}
T_e(A_{ij}) &= (A_{ij}, 1)\\
\eta_e(\theta) &= ( \log(\frac{\theta}{1-\theta}), \log(1-\theta) )
\end{align*}
and pair $ (T_w, \eta_w) $ denotes the family of the link weight distributions:

\begin{align*}
T_w(A_{ij}) &= (A_{ij}, A_{ij}^2, 1)\\
\eta_w(\theta)
&= (\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}, -\frac{\mu^2}{2\sigma^2})
\end{align*}
and $ \alpha \in [0, 1]$ is a tuning parameter that determines their relative importance,
E is the set of observed interactions,
and W is the set of weighted edges.
In the following, we use $ \alpha = 0.5 $ following the practice in \cite{aicher2014learning}.

\subsubsection{DCWBM (Degree Corrected Weighted Stochastic Block Model)}
The DCWBM is designed to incorporate node degree
by replacing pair $ (T_e, \eta_e) $ in the bWSBM with:
\begin{align*}
T_e(A_{ij}) &= (A_{ij}, -d_id_j)\\
\eta_e(\theta) &= (\log\theta, \theta)
\end{align*}
where $ d_i $ is the degree of node i \cite{aicher2014learning}.

\subsubsection{Deep learning approaches to problems related to link weight prediction}
There are a few deep learning approaches to
the node classification problem (i.e., to predict the label of a node) and 
link existence prediction problem (i.e., to predict whether a link exists).
Most of these approaches are derived from word2vec \cite{mikolov2013efficient}.
The word2vec technique is famous for using skip-gram neural net model
to learn to map every entity (word in this case) in a vocabulary to a vector
without any domain knowledge, shown in \autoref{fig:skipGram}.
For example, for a word sequence [The quick brown fox jumps over the lazy dog],
the input node corresponding to jumps is activated and the rest are deactivated;
the output node corresponding the, quick, brown, fox, over, lazy dog are activated
and the rest are deactivated.
In a corpus, every word is described/defined only by related words in its 
contexts, by implicit relations between words in word co-occurrences.
Nonetheless, after learning on many word sequences in a language corpus,
the neural net can learn from word co-occurrences and map words to 
vectors accordingly,
such that the relations between words are preserved in the word vector space 
\cite{mikolov2013distributed}.
\begin{figure}[!htb]
	\centering
	\newcommand{\layersep}{2.5cm}
	\newcommand{\vocabularySize}{8}
	\newcommand{\embeddingSize}{4}
	\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
	\tikzstyle{every pin edge}=[<-,shorten <=1pt]
	\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
	\tikzstyle{input neuron}=[neuron, fill=green!50];
	\tikzstyle{output neuron}=[neuron, fill=red!50];
	\tikzstyle{hidden neuron}=[neuron, fill=blue!50];
	\tikzstyle{annot} = [text width=4em, text centered]
	
	% Draw the input layer
	\foreach \name / \y in {1,...,\vocabularySize}
	% This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
	\node[input neuron, pin=left:$ w_\y $] (I-\name) at (0,-\y) {};
	
	% Draw the hidden layer
	\foreach \name / \y in {1,...,\embeddingSize}
	\path[yshift=-2cm]
	node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
	
	% Draw the output layer
	\foreach \name / \y in {1,...,\vocabularySize}
	\node[output neuron, pin={[pin edge={->}]right:$ w_\y $}] (O-\name) at (2*\layersep,-\y) {};
	
	% Connect the input layer with the hidden layer
	\foreach \source in {1,...,\vocabularySize}
	\foreach \dest in {1,...,\embeddingSize}
	\path (I-\source) edge (H-\dest);
	
	% Connect the hidden layer with the output layer
	\foreach \source in {1,...,\embeddingSize}
	\foreach \dest in {1,...,\vocabularySize}
	\path (H-\source) edge (O-\dest);
	
	% Annotate the layers
	\node[annot,above of=H-2] {embedding layer};
	\node[annot,above of=I-2] {input layer};
	\node[annot,above of=O-2] {output layer};
	\end{tikzpicture}	
	\caption{The skip-gram model with vocabulary size 8 and embedding size 4: learning a word vector supervised by its contexts}
	\label{fig:skipGram}
\end{figure}

An early approach is deep walk \cite{perozzi2014deepwalk}.
The goal of this approach is to map nodes to vectors.
The main idea is to reduce the problem to mapping words to vectors,
so that skip-gram model can effectively process the graph.
In this approach, a graph is sampled as a set of walks,
where each walk is a sequence of nodes.
The approach treat each node as a word, each walk as a sentence and
the graph as a natural language corpus.

A later approach is node2vec \cite{grovernode2vec},
which uses the same skip-gram model, 
but a different way to reduce the graph to a natural language corpus.
In deep walk, the walking policy is uniform random,
which means the probability of walking to each neighbor node is uniform.
However, node2vec uses a more sophisticated walking policy:
the probability to walk to next neighbor depends on
the distance this neighbor to the previous node the walk has chosen.
This policy allows the walk to spend more time exploring neighboring nodes
before walking away deep into remote and not so related nodes.

Another related approach is item2vec \cite{barkan2016item2vec}
which is designed for recommendation systems.
The approach aims to learn to map each item to a vector
and it uses the same skip-gram model.
In this approach, every item is treated as a word and
a list of items purchased together is treated as a sentence of words.
The model can therefore map items to vectors.

\subsection{Deep learning approach}
Our goal is to create a deep learning approach to link weight prediction problem.
The related works provide several insights on possible ways 
to map nodes in a graph to vectors in a continuous space,
and a number of baseline approaches to link weight prediction.
With these as inspirations,
we create a deep learning approach with a node pair as input to a neural net model
and the weight of the link connecting the nodes as its output.

\subsubsection{Model R}
We design the model in the estimator as a fully connected neural net which we call Model R (R as in "relation"), shown in Figure \ref{fig:model}.
We have considered a convolutional neural net as an alternative,
but there is no spatial property in the input node vectors
for a convolutional neural to take advantage of,
compared to the 2D array of an image where the spatial location of each pixel
has significant meaning (e.g., relative distances of pixels).

\begin{figure*}[!htb]
	\centering
	\begin{tikzpicture}[node distance=2cm]
	\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=1cm, 
	minimum height=1cm, text centered, draw=black, fill=red!30]
	\tikzstyle{process} = [rectangle, minimum width=1cm, minimum height=1cm, 
	text centered, draw=black, fill=blue!30]
	\tikzstyle{arrow} = [thick,->,>=stealth]
	\node (linearRegression) [process] {linear regression ($ f(x) = kx $)};
	\node (relu0) [process, below of=linearRegression] {rectifier ($ f(x) = \max (0, x) $)};
	\node (relu) [process, below of=relu0] {rectifier ($ f(x) = \max (0, x) $)};
	\node (linear1) [process, below of=relu, xshift=-4cm] {node vector};
	\node (linear2) [process, below of=relu, xshift=4cm] {node vector};
	\node (oneHot1) [startstop, below of=linear1] {node dictionary (node ID: node vector)};
	\node (oneHot2) [startstop, below of=linear2] {node dictionary (node ID: node vector)};
	\node (weight) [above of=linearRegression] {link weight};
	\node (output) [left of=linearRegression, xshift=-7cm] {output layer};
	\node (hidden0) [below of=output] {hidden layer};
	\node (hidden) [below of=hidden0] {hidden layer};
	\node (input) [below of=hidden] {input layer};
	\node (mapping) [below of=input] {mapping layer};
	\node (source) [below of=oneHot1] {node ID};
	\node (destination) [below of=oneHot2] {node ID};
	\draw [arrow] (source) -- (oneHot1);
	\draw [arrow] (destination) -- (oneHot2);
	\draw [arrow] (oneHot1) -- (linear1);
	\draw [arrow] (oneHot2) -- (linear2);
	\draw [arrow] (linear1) -- (relu);
	\draw [arrow] (linear2) -- (relu);
	\draw [arrow] (relu0) -- (linearRegression);
	\draw [arrow] (relu) -- (relu0);
	\draw [arrow] (linearRegression) -- (weight);
	\end{tikzpicture}
	\caption{
		Model R for a weighted graph.
		Only layers and their connections are shown,
		while the units in each layer and their connections are not shown.
		We feed every node to the estimator by feeding the ID.
		During learning,
		the estimator learns and populates the vectors in the tables.
	}
	\label{fig:model}
\end{figure*}
The model contains the following layers:
\begin{itemize}
	\item A mapping layer that maps node IDs to node vectors.
	In the node dictionary,	the key is the node ID, the value is the node vector,
	which is learned through back propagation and stochastic gradient descent.
	\item An input layer directly activated by the node dictionaries.
	Its activations are the two node vectors.
	\item Multiple fully connected hidden layers of rectified linear units
	(only two layers are shown in the figure).
	These units employ the rectifier ($ f(x) = \max (0, x) $)
	as their activation function.
	Compared to earlier popular activation functions like sigmoid function
	($ f(x) = (1 + \exp(-x))^{-1} $),
	rectifier not only simplifies and accelerates computation,
	but also eliminates vanishing gradient problems,
	and has become the most popular activation function
	for deep neural networks \cite{lecun2015deep}.
	These layers learn to extract more and more abstract weight-relevant 
	information.
	\item An output layer with a linear regression unit.
	This unit employs linear regression ($ f(x) = kx $) as its activation function.
	It learns to predict the link weight as a real number
	using abstracted weight-relevant information.
\end{itemize}
Link weights provide the information about nodes.
We fully take this property into account and design this model to learn 
complex and unobservable node information (i.e., node vectors) 
supervised by a simple and observable relation between nodes (i.e., link weight).

\subsubsection{Learning techniques}
The estimator uses the above model and a number of popular deep learning 
techniques:
\begin{itemize}
	\item Backpropagation: propagation of the error gradients from output layer 
	back to each earlier layer \cite{rumelhart1988learning}
	\item Stochastic gradient descent: the optimization that minimizes 
	the error (descending against the error gradient in weight space) for a 
	random sample in each gradient descent step \cite{lecun2012efficient}
	\item Mini-batch: the modification to stochastic gradient descent to 
	accelerate and smooth the descent by minimizing the error for a small 
	random batch of samples in each gradient descent step \cite{mairal2010online}
	\item Early stopping: the regularization used to reduce over-fitting during the iterative learning process by stopping the learning when validation error stops decreasing \cite{smale2007learning}
\end{itemize}

\subsection{Experiments}
We evaluate Model R experimentally with SBM, pWSBM, bWSBM and DCWBM as baselines,
and compare their prediction errors on several datasets.
We use the same datasets and experiment process used in a recent study of these baselines \cite{aicher2014learning}.
The results show 
that Model R can achieve much lower prediction error than the baseline models.

\subsubsection{Datasets}
The experiments use four datasets summarized in \autoref{tab:datasets}.
\begin{table*}[!htb]\centering
	\caption{The datasets used in experiments.}
	\begin{tabularx}{\textwidth}{|c|c|X|c|X|}  \hline \rowcolor{blue!40}
		Dataset & Node count & Node type & Link count & Link weight type \\ \hline
		Airport\cite{colizza2007reaction} & 500 & busiest airports in US & 5960 & number of passengers traveling from one airport to the other\\ \hline
		Collaboration\cite{pan2012world} & 226 & nations on Earth & 20616 & number of academic papers written by authors from the two connected nations \\ \hline
		Congress\cite{porter2005network} & 163  & 102nd US Congress committees & 26569 & number of shared members from the two committees \\ \hline
		Forum\cite{opsahl2009clustering}  & 1899 & users of a student social network at UC Irvine & 20291 & number of messages sent from one student to the other \\ \hline
	\end{tabularx}
	\label{tab:datasets}
\end{table*}

\subsubsection{Experiment results}
In our experiments,
Model R's error is lower than every other model on every dataset,
shown in \autoref{fig:errors}.
\begin{figure*}[!htb]\centering
	\includegraphics[width=\textwidth]{link-weight-errors}
	\caption{
		The mean squared errors of 5 models on 4 datasets:
		Model R has lower error than every other model on every dataset.
		Every error value shown here is the mean errors for the 25 trials in the experiment.
	}
	\label{fig:errors}
\end{figure*}
In this section we compare Model R with the baseline models on every dataset.
Given the dataset,
we regard ModelRError (as well as BaselineError) as a random variable
so each trial generates an example of it.
We can do a Student's t-test to justify the significance of difference between the means of variables ModelRError and BaselineError.
The mean of a variable is not the same as the mean of a sample of the variable.
More specifically, a variable can generate two samples with different sample means,
therefore two samples with different means do not imply the two variables generating them have different means.
For each dataset, we do a t-test for the two variables where the null hypothesis is that the two variables have the same mean:
\begin{align*}
\overline{ModelRError} = \overline{BaselineError}
\end{align*}
where $ \overline{X} $ is the mean of variable X.
The smaller p value is in the t-test, the more confidently we can reject the null hypothesis, i.e., accept that:
\begin{align*}
\overline{ModelRError} \neq \overline{BaselineError}
\end{align*}
Typically there is a domain specific threshold for p, e.g., 0.1 or 0.01. If p is smaller than the threshold we reject the null hypothesis.
We calculate the p value and also error reduction from baseline to Model R as:
\begin{align*}
Reduction = \frac{BaselineError - ModelRError}{BaselineError}
\end{align*}
The p value is almost 0 for all datasets and error reduction is significant,
shown in \autoref{tab:errors}.
Model R has lower error than every other model on every dataset,
reducing error by 25\% to 73\% from the best baseline model - pWSBM.
The very low p values strongly indicate the error reduction is significant.
\begin{table*}[!htb]\centering
	\caption{
		The mean squared errors with standard deviations of 5 models on 4 datasets.
	}
	\begin{tabularx}{\textwidth}{|c|X|X|X|X|X|c|c|} \hline \rowcolor{blue!40}
		Dataset & pWSBM & bWSBM & SBM & DCWBM & Model R & Reduction & p \\ \hline
		Airport & 0.0486 $ \pm $ 0.0006 & 0.0543 $ \pm $ 0.0005 & 0.0632 $ \pm $ 0.0008 & 0.0746 $ \pm $ 0.0009 & 0.013 $ \pm $ 0.001 & 73\% & 4.2e-66 \\ \hline
		Collaboration & 0.0407 $ \pm $ 0.0001 & 0.0462 $ \pm $ 0.0001 & 0.0497 $ \pm $ 0.0003 & 0.0500 $ \pm $ 0.0002 & 0.030 $ \pm $ 0.001 & 25\% & 9.1e-44 \\ \hline
		Congress & 0.0571 $ \pm $ 0.0004 & 0.0594 $ \pm $ 0.0004 & 0.0634 $ \pm $ 0.0006 & 0.0653 $ \pm $ 0.0004 & 0.036 $ \pm $ 0.003 & 35\% & 7.1e-35 \\ \hline
		Forum & 0.0726 $ \pm $ 0.0003 & 0.0845 $ \pm $ 0.0003 & 0.0851 $ \pm $ 0.0004 & 0.0882 $ \pm $ 0.0004 & 0.037 $ \pm $ 0.001 & 48\% & 4.2e-68 \\ \hline
	\end{tabularx}
	\label{tab:errors}
\end{table*}
These results show that Model R outperforms pWSBM on all these datasets.

\subsection{Conclusion}
Model R shows that deep learning can be successfully applied to link weight prediction problem.
It effectively learns complex and unobservable node information (i.e., node vectors) from simple and observable relations between nodes (i.e., link weight),
and uses that information to predict unknown link weights.
Compared to Stochastic Block Model based approaches,
this neural net model is much more accurate.
We anticipate this new approach will provide effective solutions to more
graph mining problems.

\subsection{Papers}
\begin{enumerate}
	\item ASONAM 2016, Node mapping: Link attribute prediction with neural networks (rejected).
	\item AAAI 2017, Deep Learning Approach to Collaborative Rating Prediction (rejected).
	\item IJCNN 2017, Deep Learning Approach to Link Weight Prediction (in review).
\end{enumerate}

\section{Future research}
In our future research,
we want to study deep learning approaches to the graph attribute prediction problem.
The estimator we need should learn to represent the graph in a meaningful way
and learn to predict the target graph attribute using that representation.

\subsection{Problem}
We consider the graph attribute prediction problem
in a graph where nodes and links have attributes.
The target graph attribute can be
categorical (a classification problem) or numeric (a regression problem).
For simplicity, we focus on the classification problem only.
We first look at an example of the problem,
and then give the problem definition.
Later we will discuss how to handle the graph regression problem.

\subsubsection{Problem example}
Let us look at an example of the graph attribute prediction problem
- chemical compound activity prediction shown in \autoref{fig:protein}.
In this problem, the estimator needs to learn to predict the compound activity
based on the compound structure.
More specifically, we show the estimator the training set of examples,
where the input is the graph representing the compound structure and
the output is the compound activity.
And the estimator needs to predict the unknown activity of each compound
in the testing set.
In general, the activity is a vector where each element represents
the existence of a specific type of compound activity.
A typical activity can be
an efficacy on the biomolecular target (on-target effect),
an undesired interaction with another biomolecules (off-target effect),
or an unanticipated toxic effect.
\begin{figure*}[!htb]\centering
	\includegraphics[width=\textwidth]{ProteinStructure}
	\caption{
		The chemical structure of a polypeptide macromolecule:
		Nodes and links in this graph represent atoms and bonds in the compound,
		where attributes (colors) of the nodes and links represents
		the types of the atoms and bonds.
		A polypeptide is a linear organic polymer consisting of a large number of amino-acid residues bonded together in a chain.
		However, in general, a compound can have any topology.
	}
	\label{fig:protein}
\end{figure*}

\subsubsection{Problem definition}
Now we define the graph attribute prediction problem.
\begin{itemize}
	\item Given a training set of graphs, where each graph has a known target attribute and every node and link has one attribute.
	\item Build a model y = f(x) where x is the graph and y is the target attribute of x,
	that can predict the target attribute of any graph in the testing set.
\end{itemize}

\subsection{Related work}
In our literature study on previous research,
we have found existing approaches to the graph classification problem
and deep learning approaches to problems related to the graph classification problem,
but no deep learning approaches to the graph classification problem.
In this section, we review these existing approaches.

\subsubsection{Graph kernel approach to graph classification}
This approach is similarity based \cite{kashima2003marginalized}.
The model is similar to support vector machines:
\[ y = f(x) = \sum_{i} \alpha_i K(x, x_i) \]
where the sign of $ y $ is the target attribute, $ f $ is the model,
$ x $ is the testing example,
$ x_i $'s are training examples,
$ \alpha_i $ is the target attribute level of $ x_i $,
and $ K(x, x_i) $ is the label sequence graph kernel
that measures the similarity of $ x $ and $ x_i $.
In the training stage,
the model learns an $ \alpha_i $ for each $ x_i $;
in the prediction stage,
it predicts $ y $ as 
the sum of $ \alpha_i $'s weighted by $ K(x, x_i) $'s \cite{scholkopf2001learning}.

An example kernel function is the label sequence graph kernel,
based on graph paths,
where each path is represented by the sequence of attributes of the nodes and links
on the path.
For example, in the graph shown in \autoref{fig:example},
we can represent a path from node A to node B via node C as (A, 4, C, 11, B).
In this example, the node attributes are categorical and link attributes are numeric.
In general, the node attributes and link attributes appear alternatively.
The label sequence graph kernel is defined as
\[ K(x_1, x_2) =
\sum_{h_1 \in x_1} \sum_{h_2 \in x_2} 
k(h_1, h_2) p(H = h_1 | x_1) p(H = h_2 | x_2) \]
where $ x_1, x_2 $ are two graphs,
$ h_i $ is any path in $ x_i $,
$ k(h_1, h_2) $ is the label sequence path kernel
that measures the similarity of $ h_1, h_2 $,
$ H $ is a random path variable predefined by a random walking policy $ W $, and
$ p(H = h_i | x_i) $ is the conditional probability of $ H = h_i $ given $ x_i $.
There are many implementations for $ W $ and $ k $.
An example $ W $ can be, at any node:
\begin{itemize}
	\item stop the walk with probability c (constant)
	\item walk to any neighbor with probability $ (1 - c) / D $,
	where $ D $ is the degree of the node
\end{itemize}
An example k can be Kronecker delta: 1 when the two paths are the same and 0 otherwise.
The model calculates $ K(x_1, x_2) $ as
the sum of $ k(h_1, h_2) $'s weighted by $ p(H = h_1 | x_1) p(H = h_2 | x_2) $'s.

\subsubsection{Graph boosting approach to graph classification}
This approach is feature based \cite{saigo2009gboost}.
It requires feature engineering with domain knowledge.
For example, in chemistry informatics, experts can define a set of discriminative chemical substructures as the features.
And a compound can be represented by a feature vector where each element indicates
the number of appearances of a certain substructure,
similar to bag-of-word model in natural language processing.
In general, this approach needs a predefined set of subgraphs and
to count the number of each type of subgraph existing in the graph, and
to represent the graph with a feature vector where each element is the count of a certain subgraph.
With this feature vector, standard machine learning models (including neural network models) can learn to predict the target attribute.
Many recent claims of deep learning approaches to compound activity prediction
take this approach by applying neural network as the machine learning model
\cite{unterthiner2015toxicity}, \cite{unterthiner2014deep} \cite{ramsundar2015massively}.
In this research, we do not consider them as deep learning approaches,
because they involve feature engineering with domain knowledge.

\subsubsection{Deep learning approach to spatial graph classification}
This approach takes advantage of the advancement of deep learning in image recognition by converting a spatial graph to an image and
applying standard deep learning techniques developed for image recognition \cite{wallach2015atomnet}.
This approach requires every node in the graph to have a spatial location,
 e.g., the atom coordinates in a compound.
The coordinates allow the construction of a 3D image of the compound,
shown in \autoref{fig:protein}.
In this research, we consider it as a special deep learning approach to
the graph classification problem, where the graph nodes have spatial attributes.

\subsection{Observations and possible deep learning approaches to graph classification}
Now we make observations about those existing approaches and
propose possible deep learning approaches to graph classification inspired by them.

\subsubsection{Graph kernel inspired deep learning approach}
Paths (i.e., sequences of node and link attributes) in a graph are similar to
sequences of characters in a natural language document.
So a potential approach is to reduce a graph to a document and apply deep learning techniques developed for natural language processing.
This approach is different from the previous node2vec and deep walk approaches
in that this approach does not map nodes to vectors like mapping words to vectors
as in word2vec or skip-gram model.
Instead it uses these node and link labels as raw input,
similar to the techniques used in speech recognition, image processing and
character-level text classification.
The neural net for this approach can be either recurrent or convolutional.

\subsubsection{Graph boosting inspired deep learning approach}
Subgraphs in a graph are similar to sub-images in an image.
If, instead of counting those predefined subgraphs,
we use deep learning to learn to detect and examine all discriminative subgraphs,
we may get a end-to-end deep learning approach for graph classification.
We haven not seen this deep learning approach in image recognition,
but previous research has revealed 
that humans take this approach in image comprehensions.
Experiments on eye movement \footnote{https://en.wikipedia.org/wiki/Eye\_movement}
provide evidences that humans examine images using
saccade (macro movement of gaze from a sub-image to another sub-image)
\footnote{https://en.wikipedia.org/wiki/Saccade} and
fixation (micro movement of gaze within a sub-image)
\footnote{https://en.wikipedia.org/wiki/Fixation\_(visual)}.
The human visual system is shown to be able to detect and examine all discriminative
sub-images in a sequence and comprehend the image.
Therefore, a potential approach to graph classification is
to directly detect and examine all discriminative subgraphs in a sequence.
This approach requires memory so it needs to contain recurrent neural nets.
It can also contain convolutional neural nets when examining every subgraph,
in order to examine a region in parallel.
However, it is not clear how to represent a subgraph
except sampling paths in the graph as in the graph kernel.

\subsection{Experiments}
We will evaluate the above possible deep learning approaches
experimentally and compare them against baselines including latest graph kernel approaches and graph boosting approaches.
We will conduct the experiments on multiple datasets and show
that deep learning approaches can achieve lower prediction error
than all baseline models on all datasets.
The experiments will use the following datasets:
\begin{itemize}
	\item The Karthikeyan data set \cite{karthikeyan2005general}
	\item The Bergstrom data set \cite{bergstrom2003molecular}
	\item the Huuskonen data set \cite{huuskonen2000estimation}
	\item the Delaney data set \cite{delaney2004esol}
	\item the Estrogen Receptor Binding Dataset data set \cite{tong2002development}
	\item the Androgen Receptor Binding Data data set \cite{blair2000estrogen}
\end{itemize}

\subsection{Conclusion}
We will show that deep learning can be successfully applied to graph attribute prediction problem.
It should effectively learn to extract information of a graph with arbitrary size,
and use that information to predict the target graph attribute.
Compared to baseline approaches including graph kernel and graph boosting,
deep learning approaches should be much more accurate.

\section{Summary}
In this proposal, I introduced my intended PhD research in
deep learning approaches to graph mining,
described two important types of problems in graph mining
I want to focus on and the background of these two problems,
presented my preliminary research in the graph link weight prediction problem,
and proposed my future research in the graph attribute prediction problem.
I anticipate the flexibility of deep learning will provide more accurate solutions requiring less domain knowledge to graph mining problems.

\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
