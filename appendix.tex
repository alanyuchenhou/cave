\chapter{Preliminaries and related work}

\section{Optimization, learning, training, fitting}
The concepts learning, training, fitting and optimization are very similar. They describe the process where the model slowly changes its variables (also called parameters or weights) in order to minimize the loss. The simplest learning algorithm is the gradient descent, which modifies each variable according to the derivative of the loss with respect to that variable, a.k.a., the loss gradient.
In each learning step, the model updates each variable a little bit along the direction of the gradient that reduces the loss.
The model uses the entire dataset in each learning step so the training is very expensive and not scalable to large datasets.
A more advanced learning algorithm is stochastic gradient descent, which uses a different batch of the dataset in each learning step.
In this way, the training is cheap and scalable to large datasets.
The batch size is typically less than one thousand.

\section{One hot encoding}
One hot encoding is a specific numeric vector representation for categorical entities. A one-hot vector is a one dimensional array which has one element set to 1 while all other elements set to 0. In this case, the one-hot vector with nth element set to 1 represents the nth element in the category.
In a hand written digit recognition example, the input of the model is an image of a hand written digit and the output is the actual digit the image is related to.
In this case, the output is a categorical variable with 10 possible values
\[0, 1, 2, 3 ... 9\]
which can have one hot encoding as shown in Table \ref{tab:one-hot-encoding}
\begin{table}[ht]\centering
	\caption{One hot encoding for digits.}
	\begin{tabular}{cc} \hline \rowcolor{blue!30}
		Digit & One-hot encoding \\ \hline
		0 & [1, 0, 0, 0, ... 0]       \\ \hline
		1 & [0, 1, 0, 0, ... 0]       \\ \hline
		2 & [0, 0, 1, 0, ... 0]       \\ \hline
		3 & [0, 0, 0, 1, ... 0]       \\ \hline
		... & ...       \\ \hline
		9 & [0, 0, 0, 0, ... 1]       \\ \hline
	\end{tabular}
	\label{tab:one-hot-encoding}
\end{table}
This encoding scheme has its advantages compared to other encoding schemes. For example, compared to binary encoding, output bits are completely independent from each other so that this encoding does not introduce any false relations between different entities. Notice that in terms of digit recognition, there is no relation between an two digits. In other words, all digits are meaningless entities.

\section{Linear regression}
A linear model uses a single weighted sum of features to make a prediction.
\begin{align*}
	y
	&= linear(x) \\
	&= Wx + b \\
\end{align*}
where $ y \in R^1, x \in R^d, W \in R^{1 \times d}, b \in R^1 $.
Linear models have a few advantages:
\begin{itemize}
	\item work well with large number of features
	\item simple and quick to train, debug and interpret
	\item reveal the impact each feature has on the output through weights assigned to each feature
\end{itemize}
The loss (or cost) function measures how bad the current model is, i.e., how far apart the current model is from the function to be modeled by evaluating the difference between the expected output and the predicted output.
The standard loss function for linear regression is the squared loss
\[loss(y, t) = (t - y)^2 \]
where t is the expected output.
The output of the loss function is called loss, or cost.

\section{Logistic regression}
Logistic regression is designed to predict the value of a boolean variable(a special case of categorical variable when the categorical variable has only two possible values)
\begin{align*}
	y
	&= P(t = 1 | x) \\
	&= logistic(x) \\
	&= \frac{1}{1 + exp(Wx+b)}\\
\end{align*}
where the output is exactly the probability of expected output being 1, and $ y \in R^1, x \in R^d, W \in R^{1 \times d}, b \in R^1 $.
The standard loss function for logistic regression is the logistic loss
\[ loss(y, t) = lg(1 + exp(-ty)) \]

\section{Softmax regression (multinomial logistic regression)}
Softmax regression is designed to predict the value of a categorical variable.
In the digit recognition example above, when the input of the model is an image of a hand written digit, the output should be the probabilities of the image being each digit
\[P(y = digit_i | x)\]
and the the probabilities of the output should always sum up to one because the expected output is one of the 10 possible values
\[ \sum_{digit_i \in \{0, 1, 2, ... 9\}}P(y = digit_i | x) = 1 \]
Softmax regression is commonly used as the activation function in the output layer of a neural net model to perform the final classification.
A softmax regression follows the principal that each observation in the input contribute the probability of each consequence in the output. Therefore, the probability of each consequence in the output is a weighted sum of all the observations related to it. The weight is positive if the observation is evidence in favor of the consequence and the weight is negative if the observation is evidence against the consequence.
The math form of a softmax regression is
\[ evidence_i = \sum_j W_{i, j} x_j + b_i \]
where $ W_i $ is the weights, $ b_i $ is the bias for class i, j is the index for the observations in the input, i.e., the elements in the input vector.
The final output, i.e., the probabilities y are a softmax function of the evidences
\begin{align*}
	y_i
	&= softmax(evidence_i) \\
	&= normalize(exp(evidence_i)) \\
	&= \frac{exp(evidence_i)}{\sum_j exp(evidence_j)} \\
\end{align*}
Here the softmax creates a legal probability distribution through normalization. The exponentiation of the evidence follows the intuition that the increase of evidence probability of the hypothesis exponentially instead of linearly.
More compactly, we can use matrix multiplication to represent the softmax regression as
\begin{align*}
	y
	&= softmax(x) \\
	&= \frac{exp(Wx + b)}{\sum_j exp((Wx + b)_j)} \\
\end{align*}
where $ y \in R^D, x \in R^d, W \in R^{D \times d}, b \in R^D $ and exp() is element wise.
Softmax unites introduce nonlinearities into the model.
In a typical classification task, the value with the highest probability is regarded as the predicted value.
Some of the classification tasks require a slightly different output format: top k values, where the top k values with highest probability are regarded as the output.
The standard loss function for softmax regression is the cross entropy loss
\[loss(y, t) = - \sum_i t_i lg(y_i)\]
where y is the predicted probability distribution and t is the expected probability distribution (the one-hot encoding of the output value).
Cross entropy originates from information compressing codes in information theory.
It measures how inefficient the predicted probability distributions are for predicting the expected output value.

\section{Kernel methods}
Kernel methods can increase the prediction accuracy of linear models.
There are two types of kernel methods: implicit (dual) kernel methods and explicit (primal) kernel methods.
Explicit kernel methods scales better with the dataset size in terms of training and evaluation time complexity and space complexity.
Kernel methods usually employ support vector machines as the models \cite{hofmann2008kernel}.
The advantage of these methods over linear models is that they have higher flexibility and better performance when the data points are not linearly separable.
The disadvantage is that their performance is not robust against hyperparameter changes.
The idea of kernel methods is to apply non-linear mapping from the original data space to a new high dimensional data space such that the data points mapped to the new space are linearly separable, and then apply a linear model on the mapped data points.
One such non-linear mapping example is Random Fourier Features mapping
\begin{align*}
	y 
	&= RFFM(x) \\
	&= cos(Wx + b) \\
\end{align*}
where $ y \in R^D, x \in R^d, W \in R^{D \times d}, b \in R^D $ and cosine() is element wise \cite{rahimi2008random}.
In this example, the elements in $ W $ and $ b $ are sampled from distributions such that the dot product (the similarity) of two data points in the new space approximates the radial basis function (RBF) kernel value of the two data points in the original space when D is large:
\begin{align*}
	y_1 \cdot y_2
	&= RFFM(x_1) \cdot RFFM(x_2)\\
	&\approx exp(-\frac{||x_1-x_2||^2}{2\sigma^2})\\
\end{align*}
The right hand side of the equation is the radial basis function (RBF) kernel, i.e., Gaussian kernel, one of the most widely used kernel functions in machine learning, which measures similarity in the new space.
Hyperparameter $ \sigma $ is the standard deviation of the approximated RBF kernel and controls the similarity measure used in the classification.
Generally speaking, larger D leads to better approximation, larger and more flexible model, higher classification accuracy, longer training time and higher possibility of over-fitting.

\section{Representations of data}
Neural nets use array representations of data exclusively in order to carry out numerical computation on the data. For example, a dataset representing a set of images is represented as an array with four dimensions:
\begin{enumerate}
	\item the dimension to index the image in the image set
	\item the dimension to index the row of a pixel in an image
	\item the dimension to index the column of a pixel in an image
	\item the dimension to index the color channel of a pixel
\end{enumerate}
With such a representation, an array of shape (1000, 64, 64, 3) represents a dataset of 1000 images, where each image has 64 rows and 64 columns of pixels, where each pixel has 3 color channels.
Moreover, array[3, 4, 5, 0] represent the light intensity value of image 3, row 4, column 5, channel 0.

\subsection{Representations of words}
Representations of words are very different from representations of images and audio.
Images and audio data are rich, high-dimensional data encoded as arrays of the individual raw pixel intensities for image data and power spectral density coefficients for audio data.
In image and spech recognition tasks all the available information about the entities is encoded in the data.
However, natural language data are symbolic and there is no direct way to encode words to provide useful information regarding the relationships between the symbols.
For example, images of dogs and cats illustrate their relations such as they are both animals, four-legged, furry, pets, but the words `dog' and `cat' do not illustrate these relations.

\subsubsection{Vector space models}
Vector space models represent (embed) words in a continuous space where semantically similar words are mapped (embedded) to nearby points.
It is based on the Distributional Hypothesis (words appearing in the same contexts share semantic meaning).
There are two categories of approaches leveraging this hypothesis: count-based methods and predictive methods.
Count-based methods compute the statistics of word co-occurrence counts and map them to vectors of words. Predictive methods learn to predict a word from its neighbors using learned vectors of words \cite{baroni2014don}.

\subsubsection{Word2vec}
Word2vec is an efficient predictive methods for word embedding. It has two variants: the Continuous Bag-of-Words model and the Skip-Gram model.
These two methods are very similar and the major difference is that the Continuous Bag-ofWords model predicts target words from context words, while the skip-gram model predicts context words from the target words \cite{levy2014dependency}.
The skip-gram model gains popularity over time as it tends to outperform the continuous-bag-of-words model in larger datasets.
The vectors obtained from this method are very useful as features for many canonical natural language processing tasks, such as part-of-speech tagging \cite{collobert2011natural}, named entity recognition \cite{turian2010word}.

\subsubsection{Language modeling}
In language modeling, the input of the neural net is a word history (a few previous words) and the output is the probability distribution of target word $ w_t $ (the next word)
\begin{align*}
	P(w_t | history)
	&= softmax(score(w_t, history)) \\
	&= \frac{exp(score(w_t, history))}{\sum_{w_u \in V}exp(score(w_u, history))}\\
\end{align*}
where V is the vocabulary, $ w_t $ is the target word and score is some computation of the compatibility of $ w_t $ and history.
The output is produced by softmax regression and the target variable is a categorical variable, we can use cross-entropy loss in the training.
The softmax computation is very expensive because it needs to compute the score for each word in the vocabulary at every training step so this computation does not scale well when the vocabulary is big because the time complexity is linear with respect to the vocabulary size
\[T = O(|vocabulary|)\]
So in practice, training usually approximates the softmax using negative sample technique where only sample of words beside the target word (negative words) in the vocabulary are used in the softmax calculation
\begin{align*}
	P(w_t | history)
	&= softmax(score(w_t, history)) \\
	&= \frac{exp(score(w_t, history))}{\sum_{w_u \in V}exp(score(w_u, history))}\\
	&\approx \frac{exp(score(w_t, history))}{\sum_{w_u \in V_n \cap {w_t}}exp(score(w_u, history))}\\
\end{align*}
where $ V_n $ is the negative word sample out of the vocabulary.
This approximation makes the cross-entropy loss computation much faster \cite{mnih2013learning}
\begin{align*}
	loss(y, t)
	&= - \sum_i t_i log(y_i)\\
	&= - \sum_i t_i log(P(w_t | history))\\
\end{align*}
Now the time complexity is linear with respect to the negative sample size instead of the vocabulary size, which is a small constant.
\[T = O(|sample|) = O(1)\]

\section{Convolutional neural net}
A convolutional neural net is a neural net that uses convolution layers and max pooling layers \cite{krizhevsky2009learning}. Typically, it consists of a stack of convolution modules. Each module consists of a convolution layer followed by a pooling layer.
The last convolution module is followed by a number of fully connected layers that perform classification.
Convolutional neural nets are the current state-of-the-art model architecture for image classification tasks \cite{szegedy2015going} \cite{ioffe2015batch} \cite{szegedy2016rethinking}.
They apply a series of filters to the raw pixel data of an image to extract and learn higher and higher level features and use them  for classifications.
It is a very common neural net in speech recognition applications as well.
It is very different from fully connected neural net in that each unit in each layer is only connected to the units in a small local region of the previous layer.
This design takes advantage of the locality of the activations of units in each layer.
For example, in image recognition applications, activations of units close to each other represent light intensities in a local spacial region; in speech recognition applications, activations of units close to each other represent sound intensities in a local temporal or frequency region.
In a convolutional neural net, it is common for each unit to have multiple activation channels, where each activation channel corresponds an output channel.
For example, light intensity pixel can have 3 channels to store light intensities at 3 different light frequency - red, green and blue.

\subsection{Convolution layer}
A convolution layer extracts features of the object from the activation of its previous layer.
Each unit in a convolution layer is connected to a small region in the previous layer, such as a 4x4 square patch of 16 units, and uses their activations as input.
This dimension of the patch is also called convolution kernel size.
For example, a unit in a convolution layer can have an input of a 4x4 patch of unites where each unit has 3 channels, while it can have an output of 32 channels where each channel is a different convolution of the input.
In this case, the input of this unit is an array of shape (4, 4, 3) and output is an array of shape (1, 1, 32).
Assuming the dataset contains 1000 images of resolution 64x64, the input of the first convolution layer should be an array of shape
(1000, 64, 64, 3).
Assuming the stride of the convolution layer is 1 and padding is performed (the convolution layer add 0 values to the edges of the input sensor to preserve the width and height of the previous layer in the output), its output should be an array of shape (1000, 64, 64, 32).
Some of the convolution layers can have strides larger than one, resulting a layer with reduced size and less units.
For example, if we change the stride of the above convolution layer to 4, its output will become an array of shape (1000, 16, 16, 32).
Most of the convolutional neural nets use rectified linear units (ReLU) so the convolution is
\begin{align*}
	y
	&= relu(x) \\
	&= max(0, Wx+b) \\
\end{align*}
where $ y \in R^n, x \in R^{a \times b}, W \in R^{n \times a \times b}, b \in R^n , 0 \in 0^n $, n is the number of filters in this layer, a is the filter kernel width, b is the filter kernel height, and max() is element wise.
ReLU unites introduce nonlinearities into the model.
Typically, each unit in a convolution layer computes multiple features from the input patch of units.
Each one of these features is obtained by a specific convolution filter.

\subsection{Max pooling layer}
A max pooling layer reduces the features extracted from its previous layer.
More specifically, it downsamples the image data extracted by the convolutional layer to reduce the dimensionality of the feature map to decrease model complexity and processing time.
The connections between a max pooling layer and its previous layer is very similar to that between a convolution layer and its previous layer.
Typically, each unit in a max pooling layer extracts the max features from the input patch of units.
For example, a unit in a max pooling layer can have an input of a 4x4 patch of units where each unit has 32 channels, and an output of 32 channels where each channel is the maximum of all the corresponding output channels from the 4x4 patch.
In this case, the input of this unit is an array of shape (4, 4, 32) and output is an array of shape (1, 1, 32).
Assuming the input of the max pooling layer is an array of shape (1000, 16, 16, 32), its output should be an array of shape (1000, 4, 4, 32).
The max pooling function is simply
\[ y = max(x) \]
It keeps the maximum of values for each feature in the feature map and discards all others.

\subsection{Fully connected layer}
A fully connected layer usually performs classification on the features extracted by convolution layers and downsampled by the pooling layers.
In a fully connected layer, every unit is connected to every unit in the previous layer.

\subsection{Dropout}
Dropout is a common technique in convolutional neural nets to reduce over fitting \cite{srivastava2014dropout}. This technique forces the model to turn off each unit with a certain probability during each training step while turn on all units during testing.
The goal is to encourage each unit to work independently and extract diverse features, instead of depending on each other and extract similar features.
Dropout is most useful when the model complexity is high, i.e. when the neural net is very large.

\section{Recurrent neural net}
Recurrent neural nets (RNN) are very effective in language modeling.
Language modeling is an important problem because it is the key to many interesting problems such as speech recognition, machine translation and image captioning \cite{zaremba2014recurrent}.
To keep things simple, we assume all the words in the corpus are word IDs (integers).
Recurrent neural net consists of multiple layers of LSTM (Long Short Term Memory) cells. Each LSTM cell processes one word at a time and computes the probability distribution of the next word.
During evaluation stage, the memory state of the model is initialized with zeros and gets updated after reading each word.
Theoretically, the output and state of an RNN depends on arbitrarily old inputs, making backpropagation computation in the learning process difficult.
Practically, to simplify the computation, the model is approximated by an unrolled version with a finite number of LSTM inputs and outputs.
The loss function is the cross entropy loss.

\section{Transfer learning}
Transfer learning is a technique that retrains the final layer of a fully trained model (trained on a dataset) on a new dataset while leaving all other layers unchanged \cite{donahue2014decaf}. Models obtained with this technique do not have prediction accuracy as good as models trained from scratch, but they are very fast to train and effective enough for many applications. For example, in image recognition, the new dataset can contain images of completely different entities from the old dataset, but the model can still quickly learn to recognize these new entity classes. The reason behind this is that lower layers in the model have been trained to recognize many basic objects and shapes so that they can be reused to recognize the same basics objects and shapes in the new datasets.
The input to the model are the word embeddings which are randomly initialized and learned during the training.
