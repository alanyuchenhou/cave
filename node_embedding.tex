\documentclass{article}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{float}
\begin{document}
\lstset{language=python, tabsize=4}
\title{Node mapping}
\author{Yuchen Hou \and Larry Holder}
\maketitle

\begin{abstract}
	We present node mapping, a technique created for link attribute prediction tasks, powered by neural networks.
	This technique builds a neural net model, which learns to map every node in 
	a graph to a node vector of real numbers and learns to predict the target 
	link attribute using these node vectors.
	Every node vector contains information about the node related to the
	prediction task.
	The main idea is to convert nodes (categorical variables) to vectors 
	(numerical variables), as neural nets can directly handle only numerical 
	variables.
\end{abstract}

\section{Introduction}

\subsection{Background}
We have seen the wide spread success of neural networks since 2010, as they are becoming state-of-the-art models in a growing number of application domains of machine learning, (e.g., speech recognition\cite{hannun2014deep}, image recognition\cite{simonyan2014very}, and natural language processing\cite{yao2013recurrent}).
These neural net models not only achieved higher prediction accuracy than traditional models, but also require much less domain knowledge and engineering.
In 2016, Google AlphaGo AI - powered by neural net and Monte Carlo tree search - defeated go champion Lee Sedol, demonstrated how a statistical approach can complement logical approaches in solving challenging problems\cite{silver2016mastering}.
Specifically, in a game like go with simple rules but too many possible configurations to explore, neural net models achieve good results with reasonable computing resources, which are easily exhausted by algorithms solely powered by search.

\subsection{Motivation}
As neural nets have demonstrated their power in many domains, we naturally wonder if we can apply them in prediction tasks in graph mining domain, and what kind of techniques are helpful in using their power.
There are existing techniques with promising results already.
Two earlier attempts were graph neural nets and relational neural nets\cite{scarselli2009graph}, where neural nets are incorporated into a traditional iterative graph information propagation approach.
A later attempt was deep walk\cite{perozzi2014deepwalk}, where a graph is reduced to a natural language corpus so that existing neural net models designed for natural language processing can handle the graph.
We find these techniques complex, indirect and unable to fully use rich graph information in real applications, e.g. making recommendations for social network users based on user activities like messaging friends, commenting articles, and rating movies.
However, these attempts all include an essential step of applying neural net in graphs: converting graph to numerical variables.
This step is essential in other application domains as well, because neural nets cannot directly handle non-numerical variables.

\subsection{Goal}
We want to create a technique to predict link attributes in a graph using a neural net model. The graph can have any topology and its links can have any number of numerical(including boolean) attributes. The technique should build a neural net model, which learns to represent the graph in a meaningful way and to predict the target link attribute using the representation it learns.

\section{Observations and approach}
In order to understand how a neural net can handle prediction tasks in graphs, we take an overview on 3 types entities in these 3 domains and in graph mining. \autoref{tab:domains} provides a summary of these entities, their representations in neural nets and inter-entity relation examples in these domains.
\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{ |c|c|c|X| }
		\hline domain & entity & representation & relations to other entities 
		\\ 
		\hline image recognition & image & 2D light intensity array & NA \\ 
		\hline speech recognition & utterance/spectrogram & 2D sound intensity array & NA \\ 
		\hline natural language & word & word vector & relations to other words \\ 
		\hline graph & movie & node vector & directed by director, etc \\ 
		\hline graph & user & node vector & rate movies, etc \\
		\hline
	\end{tabularx}
	\caption{A summary of various types of entities, their numerical representations and inter-entity relations in different domains: images and utterances can be directly represented by 2D numerical arrays, but have no strong relations to other images or utterances; words and various types of nodes in graphs can represented by 1D numerical arrays(which will be introduced below), and have strong relations to other words and nodes. Notice that the representations for all the entities are numerical arrays, because neural nets rely on neurons' activations and communications, which are both numerical.}
	\label{tab:domains}
\end{table}

\subsection{Entity representations in different domains}
In \autoref{tab:domains}, the entities listed in the lower rows have increasing representation complexities. Images and utterances(spectrograms) can be naturally represent as 2D numerical arrays, which makes them easy for neural net to process. Words are categorical variables and consequently harder, but they can be mapped to 1D numerical arrays (i.e. word vectors) by word2vec technique \cite{mikolov2013efficient} so neural nets can still process them.
If we can map nodes to node vectors in a similar way, neural nets might be able to handle challenging prediction tasks in graph domain as well.
Now the question is how to achieve this representation. Interestingly, it turns out neural net itself has the capability to do it.

\subsection{Learning word vectors and node vectors}
In word2vec, the mapping from words to vectors are not engineered using any domain knowledge, but learned by a shallow neural net from the relations between words manifested in word co-occurrences in a corpus. This learning is possible because every word is described/defined only by its relations with related words and there is no other information we can get from the word itself or from the corpus(here we ignore the information provided by the character sequence and treat words as atomic, because this phenomenon doesn't exist in graphs).

\subsubsection{Sequence, word and context}
\begin{itemize}
	\item sequence = [guard, peace, and, justice, in, the, universe]
	\item word = [justice]
	\item context of radius 3 = [guard, peace, and, in, the, universe]
\end{itemize}
\subsubsection{Co-occurrences express relations and attributes}
\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{|c|X|c|} \hline
		co-occurrence & relation & attribute \\ \hline
		(guard, justice) & justice is protected because it's important and evil 
		wants to destroy it & value, vulnerability \\ \hline
		(peace, justice) & justice is morally good and desirable, like peace & 
		virtue \\ \hline
		... & ... & ... \\ \hline
	\end{tabularx}
	\caption{Attributes and relations about word [justice]: the information of 
	a word is implicitly expressed by words in its contexts.}
	\label{tab:cooccurrences}
\end{table}
\subsubsection{Learn a word from its contexts}
\begin{itemize}
	\item learn [Jedi]: a member of the mystical knightly order in the Star 
	Wars films, trained to guard peace and justice in the universe (Google 
	Dictionary)
	\item learn every word supervised by the words in its contexts
\end{itemize}

\subsection{Implementation}

\subsubsection{Skip-gram model}\begin{figure}[H]
	\centering
	\newcommand{\layersep}{2.5cm}
	\newcommand{\vocabularySize}{8}
	\newcommand{\mappingSize}{4}
	\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
	\tikzstyle{every pin edge}=[<-,shorten <=1pt]
	\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
	\tikzstyle{input neuron}=[neuron, fill=green!50];
	\tikzstyle{output neuron}=[neuron, fill=red!50];
	\tikzstyle{hidden neuron}=[neuron, fill=blue!50];
	\tikzstyle{annot} = [text width=4em, text centered]
	
	% Draw the input layer
	\foreach \name / \y in {1,...,\vocabularySize}
	% This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
	\node[input neuron, pin=left:$ w_\y $] (I-\name) at (0,-\y) {};
	
	% Draw the hidden layer
	\foreach \name / \y in {1,...,\mappingSize}
	\path[yshift=-2cm]
	node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
	
	% Draw the output layer
	\foreach \name / \y in {1,...,\vocabularySize}
	\node[output neuron, pin={[pin edge={->}]right:$ w_\y $}] (O-\name) at 
	(2*\layersep,-\y) {};
	
	% Connect the input layer with the hidden layer
	\foreach \source in {1,...,\vocabularySize}
	\foreach \dest in {1,...,\mappingSize}
	\path (I-\source) edge (H-\dest);
	
	% Connect the hidden layer with the output layer
	\foreach \source in {1,...,\mappingSize}
	\foreach \dest in {1,...,\vocabularySize}
	\path (H-\source) edge (O-\dest);
	
	% Annotate the layers
	\node[annot,above of=H-2] {mapping layer};
	\node[annot,above of=I-2] {input layer};
	\node[annot,above of=O-2] {output layer};
	\end{tikzpicture}
	
	\caption{The skip-gram model with vocabulary size 8 and mapping size 4: 
	learning a word vector supervised by its contexts}
	\label{fig:skipGram}
\end{figure}
\begin{itemize}
	\item linear units in the mapping layer: $ y_i = f(x_i) = w_i x_i $
	\item softmax units in the output layer: $ y_i = f_i(x) = \frac{exp(w_i 
	\cdot x)}{\sum_{i = 1}^{n}exp(w_i \cdot x)} $
	\item cross entropy loss: $ loss = H(t, y) = - \sum_{i = 1}^{n} t_i 
	log(y_i) $
\end{itemize}

\subsection{Learning outcome}
\begin{enumerate}
	\item word vectors stored in weights in mapping layer
	\item word vector distance $ \approx $ word semantic similarity
	\item word vector differences $ \approx $ word semantic relation
\end{enumerate}
\href{https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html}{word2vec
 visualization} by TensorFlow project

\section{Node mapping}

\subsection{The goal}
\begin{itemize}
	\item represent nodes with meaningful numerical vectors for neural nets
\end{itemize}
\begin{table}[H]
	\centering
	\begin{tabularx}{0.5\textwidth}{|c|c|X|} \hline
		ID & node & node vector \\ \hline
		1 & A & [2.3, 564, -9.5 ... 3] \\ \hline
		2 & B & [76, -342.2, 0.3 ... 4.2] \\ \hline
		3 & C & [-345, -834, 0.3 ... 34] \\ \hline
		... & ... & ... \\ \hline
		n & $ node_n $ & $ [x_1, x_2, x_3 ... x_d] $ \\ \hline
	\end{tabularx}
	\caption{Node vectors sorted by the node ID in a graph: each node is 
	represented by a vector of size d.}
	\label{tab:node}
\end{table}

\subsection{Application scenario}

\subsubsection{The dataset}
\begin{table}[H]
	\centering
	\begin{tabularx}{0.5\textwidth}{|c|X| }  \hline
		nodeID & labels \\ \hline
		0 & [ture, false, false... true] \\ \hline
		1 & [ture, true, false... false] \\ \hline
		2 & [false, false, false... true] \\ \hline
		... & ... \\ \hline
		n & [ture, false, true... true] \\ \hline
	\end{tabularx}
	\caption{The node set: a node can have a number of labels.}
\end{table}
\begin{table}[H]
	\centering
	\begin{tabularx}{0.5\textwidth}{|X|X|}  \hline
		sourceID & destinationID \\ \hline
		0 & 4355 \\ \hline
		0 & 987876 \\ \hline
		2 & 324 \\ \hline
		... & ... \\ \hline
		n & x \\ \hline
	\end{tabularx}
	\caption{The link set: a link has no attributes}
\end{table}
\begin{itemize}
	\item dataset: graph = (nodes, links)
	\item link attribute: none
	\item node attribute: labels(list of booleans)
\end{itemize}

\subsubsection{Prediction task: node attribute prediction}
\begin{enumerate}
	\item input: X = node, node in nodes
	\item output: Y = [y[0], y[1], y[2] ... y[n-1]], y[i] in [true, false]
\end{enumerate}

\subsection{Observations and the approach}
\begin{table}[H]
	\centering
	\begin{tabularx}{\textwidth}{ |c|c|X| } \hline
		aspect  & natural language processing & graph mining \\ \hline
		dataset & corpus & graph \\ \hline
		basic entities & words & nodes \\ \hline
		entity collection & vocabulary & node set \\ \hline
		relations & co-occurrences (implicit) & links (explicit) \\ \hline
		entity sequences & sentences & random walks \\ \hline
		representation & word vector & node vector \\ \hline
	\end{tabularx}
	\caption{A comparison of natural language processing and graph mining from 
	several aspects: from word mapping to node mapping.}
	\label{tab:wordVSnode}
\end{table}

\subsection{Implementation}

\subsubsection{Node mapping stage}
\begin{itemize}
	\item model: skip-gram model
	\item node sequence generation: sampled random walks on the graph
	\item weights update: weights in all layers
	\item learning outcome: node vectors stored in weights in mapping layer
\end{itemize}
\subsubsection{Label prediction stage}
\begin{itemize}
	\item model: adjust output layer size to label set size
	\item weights update: weights in output layer only
\end{itemize}
\begin{figure}[H]
	\centering
	\newcommand{\layersep}{2.5cm}
	\newcommand{\vocabularySize}{8}
	\newcommand{\mappingSize}{4}
	\newcommand{\labelSetSize}{2}
	\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
	\tikzstyle{every pin edge}=[<-,shorten <=1pt]
	\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
	\tikzstyle{input neuron}=[neuron, fill=green!50];
	\tikzstyle{output neuron}=[neuron, fill=red!50];
	\tikzstyle{hidden neuron}=[neuron, fill=blue!50];
	\tikzstyle{annot} = [text width=4em, text centered]
	
	% Draw the input layer
	\foreach \name / \y in {1,...,\vocabularySize}
	% This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
	\node[input neuron, pin=left:$ node_\y $] (I-\name) at (0,-\y) {};
	
	% Draw the hidden layer
	\foreach \name / \y in {1,...,\mappingSize}
	\path[yshift=-2cm]
	node[hidden neuron] (H-\name) at (\layersep,-\y) {};
	
	% Draw the output layer
	\foreach \name / \y in {1,...,\labelSetSize}
	\node[output neuron, pin={[pin edge={->}]right:$ label_\y $}] (O-\name) at 
	(2*\layersep,-\y-3) {};
	
	% Connect the input layer with the hidden layer
	\foreach \source in {1,...,\vocabularySize}
	\foreach \dest in {1,...,\mappingSize}
	\path (I-\source) edge (H-\dest);
	
	% Connect the hidden layer with the output layer
	\foreach \source in {1,...,\mappingSize}
	\foreach \dest in {1,...,\labelSetSize}
	\path (H-\source) edge (O-\dest);
	
	% Annotate the layers
	\node[annot,above of=H-2] {mapping layer};
	\node[annot,above of=I-2] {input layer};
	\node[annot,above of=O-2] {output layer};
	\end{tikzpicture}
	\caption{The label prediction model with node set size 8, mapping size 4 
	and label set size 2: learning label prediction.}
	\label{fig:label}
\end{figure}

\subsection{Strengths of this approach}

\subsubsection{Representing nodes as node vectors}
\begin{table}[H]
	\centering
	\begin{tabularx}{0.5\textwidth}{ |c|X|X| } \hline
		aspect  & node & link \\ \hline
		complexity & high & low \\ \hline
		observability & low & high \\ \hline
		example & user & activities \\ \hline
	\end{tabularx}
	\caption{A comparison of node and link with respect to their complexity and 
	attribute observability: what one does tells us what kind of person he is.}
	\label{tab:nodesVSlinks}
\end{table}

\subsubsection{Online learning}
\begin{itemize}
	\item low memory requirement
	\item incremental model construction
	\item suitable for streaming graph
\end{itemize}

\subsection{Weaknesses of this approach}

\subsubsection{Node sequence generation}
\begin{enumerate}
	\item remote nodes in a deep walk is not strongly related to the root
	\item skip-gram's indifference to important node ordering
	\item power law node frequency distributions
\end{enumerate}

\subsubsection{Unable to use rich attributes in graph}
\begin{itemize}
	\item nodes and links have attributes
	\item attributes can provide more information than topology
\end{itemize}

\begin{figure}[H]
	\centering
	\newcommand{\layersep}{8}
	\newcommand{\vocabularySize}{3}
	\newcommand{\mappingSize}{5}
	\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
	\tikzstyle{every pin edge}=[<-,shorten <=1pt]
	\tikzstyle{neuron}=[circle, draw, minimum size=20]
	\tikzstyle{input neuron}=[neuron];
	\tikzstyle{output neuron}=[neuron];
	\tikzstyle{hidden neuron}=[neuron];
	\tikzstyle{annot} = [text width=4em, text centered]
	
	% Draw the input layer
	\foreach \name / \y in {1,...,\vocabularySize}
	\node[input neuron] (I-\name) at (0,-2*\y) {$ user_\name $};
	
	% Draw the hidden layer
	\foreach \name / \y in {1,...,\mappingSize}
	\path[yshift=2cm]
	node[hidden neuron] (H-\name) at (\layersep,-2*\y) {$ movie_\name $};
	
	% Connect the input layer with the hidden layer
	\foreach \source in {1,...,\vocabularySize}
	\foreach \dest in {1,...,\mappingSize}
	\path (I-\source) edge (H-\dest);
	\end{tikzpicture}
	
	\caption{The weakness of solely relying on topology and ignoring other rich 
	attributes: unable to discriminate nodes.}
	\label{fig:weakness}
\end{figure}

\section{Node mapping for graphs with rich attributes}

\subsection{Advanced features/strengths}
\begin{enumerate}
	\item comprehensive learning: exploiting rich node and link attributes
	\item multi-task prediction: learn to predict any node or link attribute
\end{enumerate}

\subsection{Application scenario}

\subsubsection{The dataset}
\begin{table}[H]
	\centering
	\begin{tabularx}{0.5\textwidth}{|X|X| }  \hline
		userID & french \\ \hline
		0 & false \\ \hline
		1 & false \\ \hline
		2 & true \\ \hline
		... & ... \\ \hline
		n & false \\ \hline
	\end{tabularx}
	\caption{The node set: a user has 1 boolean attribute - french}
	\label{tab:user}
\end{table}
\begin{table}[H]
	\centering
	\begin{tabularx}{0.5\textwidth}{|X|X|X| }  \hline
		userID & movieID & rating \\ \hline
		0 & 4355 & 4 \\ \hline
		0 & 987876 & 7 \\ \hline
		2 & 324 & 2 \\ \hline
		... & ... & ... \\ \hline
		n & x & 9 \\ \hline
	\end{tabularx}
	\caption{The link set: a link has 1 numerical attribute - rating}
	\label{tab:rating}
\end{table}
\begin{itemize}
	\item dataset: graph = (\{users and movies\}, ratings)
	\item movie attribute: none
	\item user attribute: french(boolean)
	\item link attribute: rating(numerical)
\end{itemize}

\subsubsection{Prediction task: node attribute prediction}
\begin{enumerate}
	\item input: X = user, user in users
	\item output: Y = french, french in [true, false]
\end{enumerate}

\subsubsection{Prediction task: link attribute prediction}
\begin{enumerate}
	\item input: X = (user, movie), user in users, movie in movies
	\item output: Y = rating, rating in range(1, 10)
\end{enumerate}

\subsection{Observations and the approach}
\begin{enumerate}
	\item french: information about the user
	\item rating: information about the user and the movie
	\item learning: supervised by both attributes
\end{enumerate}

\subsection{Implementation}

\subsubsection{Multi-task model}
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[node distance=2cm]
	\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right 
	angle=110, minimum width=0cm, minimum height=1cm, text centered, 
	draw=black, fill=blue!30]
	\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, 
	text centered, draw=black, fill=green!30]
	\tikzstyle{arrow} = [thick,->,>=stealth]
	\node (linearRegression) [io] {linear regression};
	\node (logisticRegression) [io, right of=linearRegression, xshift=4cm] 
	{logistic regression};
	\node (relu1) [process, below of=logisticRegression, yshift=-2cm] 
	{ReLU[d=16]};
	\node (relu2) [process, below of=linearRegression] {ReLU[d=32]};
	\node (relu3) [process, below of=relu2] {ReLU[d=32]};
	\node (linear1) [process, below of=relu1] {linear[d=64]};
	\node (linear2) [process, below of=relu3] {linear[d=64]};
	\node (oneHot2) [io, below of=linear1] {one-hot[d=n]};
	\node (oneHot1) [io, below of=linear2] {one-hot[d=m]};
	\node (rating) [above of=linearRegression] {rating};
	\node (french) [above of=logisticRegression] {french};
	\node (output) [left of=linearRegression, xshift=-2cm] {output layer};
	\node (hidden2) [below of=output] {hidden layer};
	\node (hidden1) [below of=hidden2] {hidden layer};
	\node (mapping) [below of=hidden1] {mapping layer};
	\node (input) [below of=mapping] {input layer};
	\node (movie) [below of=oneHot1] {movie};
	\node (user) [below of=oneHot2] {user};
	\draw [arrow] (movie) -- (oneHot1);
	\draw [arrow] (user) -- (oneHot2);
	\draw [arrow] (oneHot2) -- (linear1);
	\draw [arrow] (oneHot1) -- (linear2);
	\draw [arrow] (linear1) -- (relu1);
	\draw [arrow] (linear1) -- (relu3);
	\draw [arrow] (linear2) -- (relu3);
	\draw [arrow] (relu3) -- (relu2);
	\draw [arrow] (relu1) -- (logisticRegression);
	\draw [arrow] (relu2) -- (linearRegression);
	\draw [arrow] (linearRegression) -- (rating);
	\draw [arrow] (logisticRegression) -- (french);
	\end{tikzpicture}	
	\caption{The multi-task prediction model with movie set size m, user set 
	size n, mapping size 64: learning rating and french attribute prediction.}
	\label{fig:multiTask}
\end{figure}
\begin{itemize}
	\item mapping layer with linear units: $ y = f(x) = w x $
	\item hidden layers with ReLU(rectified linear units): $ y = f(x) = max(0, 
	w x) $
	\item output layer with linear regression unit: $ y = f(x) = w x $
	\item output layer with logistic regression unit: $ y = f(x) = \frac{1}{1 + 
	exp(-w x)} $
	\item hinge loss for logistic regression: $ loss = l(t, y) = max(0, 1 - ty) 
	$
	\item squared error for linear regression: $ loss = l(t, y) = (t - y)^2 $
\end{itemize}

\section{Conclusions}
This technique has the following strengths:
\begin{enumerate}
	\item low complexity: it is simple and scalable
	\item high flexibility: it can learn from and predict rich node and link attributes
	\item high efficiency: it is very streamlined online learning without any expensive graph operations(not even neighbor node lookup or random walk)
\end{enumerate}
and the following weaknesses:
\begin{enumerate}
	\item unable to handle non-numerical attributes: e.g., a message from user A to user B (a link from A to B) or an article (a node) contain texts (string attributes), which this technique cannot directly handle
\end{enumerate}
\begin{itemize}
	\item node mapping: neural net approach for learning vector 
	representations of nodes
	\item online learning: good for streaming graph
	\item node mapping for graphs with rich attributes: low complexity, 
	comprehensive learning and multi-task prediction
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Thank-you-word-cloud}
	\caption{ 
	\href{https://commons.wikimedia.org/wiki/File:Spectrogram-19thC.png}{Thank 
	you word cloud}(Aquegg / Wikimedia Commons / Attribution-Share Alike 4.0 
	International)}
	\label{fig:Thank-you-word-cloud}
\end{figure}

\bibliographystyle{acm}
\bibliography{references}

\end{document}
